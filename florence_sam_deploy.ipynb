{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import importlib"
      ],
      "metadata": {
        "id": "VLHKdLb4d6SL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt8ZlSslFZFn",
        "outputId": "57fb75b4-818c-4af3-81b9-5c0c4a5e4da7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kclL8bxODRlD",
        "outputId": "fa97ec62-a1b0-4879-e295-9121602a3581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'florence-sam'...\n",
            "remote: Enumerating objects: 91, done.\u001b[K\n",
            "remote: Total 91 (delta 0), reused 0 (delta 0), pack-reused 91 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (91/91), 17.10 KiB | 416.00 KiB/s, done.\n",
            "Filtering content: 100% (7/7), 1.46 GiB | 63.15 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/spaces/SkalskiP/florence-sam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd florence-sam"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkYEWz0MDXlz",
        "outputId": "eda45e84-4b4c-4c0f-b273-1c4d41e7ee4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/florence-sam\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Library        | Version     |\n",
        "|----------------|-------------|\n",
        "| tqdm           | 4.67.1      |\n",
        "| einops         | 0.8.1       |\n",
        "| spaces         | 0.39.0      |\n",
        "| timm           | 1.0.19      |\n",
        "| transformers   | 4.54.1      |\n",
        "| samv2          | 0.0.4       |\n",
        "| gradio         | 5.39.0      |\n",
        "| supervision    | 0.26.1      |\n",
        "| opencv-python  | 4.12.0.88   |\n",
        "| pytest         | 8.4.1       |\n"
      ],
      "metadata": {
        "id": "VVdO2JjRhFx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "tqdm==4.67.1\n",
        "einops==0.8.1\n",
        "spaces==0.39.0\n",
        "timm==1.0.19\n",
        "transformers==4.54.1\n",
        "samv2==0.0.4\n",
        "gradio==5.39.0\n",
        "supervision==0.26.1\n",
        "opencv-python==4.12.0.88\n",
        "pytest==8.4.1\n",
        "\"\"\".strip())"
      ],
      "metadata": {
        "id": "BmUB1qq4hvxt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i '/import spaces/d' app.py\n",
        "!sed -i '/@spaces.GPU/d' app.py\n",
        "!sed -i 's/imports.remove(\"flash_attn\")/if \"flash_attn\" in imports: imports.remove(\"flash_attn\")/' \"utils/florence.py\""
      ],
      "metadata": {
        "id": "yfqJIiN7DTz4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "id": "Ax9j-lSvD2SF"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i '/launch(/s/launch(\\(.*\\))/launch(share=True, \\1)/' app.py"
      ],
      "metadata": {
        "id": "LecUNwNscSYa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i '/generated_ids = model.generate(/a\\        use_cache=False,' \"utils/florence.py\""
      ],
      "metadata": {
        "id": "K10baGpZeiR_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app_path = \"app.py\"\n",
        "\n",
        "with open(app_path, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "patch = [\n",
        "    \"\\n\",\n",
        "    \"# Florence patch to fix SDPA error\\n\",\n",
        "    \"import subprocess\\n\",\n",
        "    \"try:\\n\",\n",
        "    \"    modeling_path = subprocess.check_output([\\n\",\n",
        "    \"        \\\"find\\\", os.path.expanduser(\\\"~/.cache/huggingface/\\\"),\\n\",\n",
        "    \"        \\\"-type\\\", \\\"f\\\", \\\"-name\\\", \\\"modeling_florence2.py\\\"\\n\",\n",
        "    \"    ], text=True).strip()\\n\",\n",
        "    \"    if os.path.exists(modeling_path):\\n\",\n",
        "    \"        with open(modeling_path, \\\"r\\\") as f:\\n\",\n",
        "    \"            content = f.read()\\n\",\n",
        "    \"        if \\\"_supports_sdpa = False\\\" not in content:\\n\",\n",
        "    \"            subprocess.run([\\n\",\n",
        "    \"                \\\"sed\\\", \\\"-i\\\",\\n\",\n",
        "    \"                \\\"/class Florence2ForConditionalGeneration/a\\\\\\\\        _supports_sdpa = False\\\",\\n\",\n",
        "    \"                modeling_path\\n\",\n",
        "    \"            ])\\n\",\n",
        "    \"            print(\\\"Patched _supports_sdpa = False into:\\\", modeling_path)\\n\",\n",
        "    \"        else:\\n\",\n",
        "    \"            print(\\\"Already patched:\\\", modeling_path)\\n\",\n",
        "    \"    else:\\n\",\n",
        "    \"        print(\\\"Florence file not found.\\\")\\n\",\n",
        "    \"except Exception as e:\\n\",\n",
        "    \"    print(\\\"Error during Florence patching:\\\", e)\\n\"\n",
        "]\n",
        "\n",
        "injection_point = None\n",
        "for i, line in enumerate(lines):\n",
        "    if \"FLORENCE_MODEL, FLORENCE_PROCESSOR = load_florence_model(device=DEVICE)\" in line:\n",
        "        injection_point = i + 1\n",
        "        break\n",
        "\n",
        "if injection_point:\n",
        "    new_lines = lines[:injection_point] + patch + lines[injection_point:]\n",
        "    with open(app_path, \"w\") as f:\n",
        "        f.writelines(new_lines)\n",
        "    print(\"Patch successfully injected into app.py\")\n",
        "else:\n",
        "    print(\"Could not find injection point.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_1YgZxV8Alt",
        "outputId": "2bb8a312-af69-45aa-fc14-a7d5b269314a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patch successfully injected into app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python app.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-O86_YDD4NR",
        "outputId": "49bdb0e2-8785-4de5-a508-4ac3362430ad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-08-07 14:38:53.719853: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1754577533.750063    7734 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1754577533.756498    7734 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1754577533.772516    7734 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754577533.772549    7734 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754577533.772553    7734 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1754577533.772556    7734 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-08-07 14:38:53.777359: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Already patched: /root/.cache/huggingface/modules/transformers_modules/microsoft/Florence-2-base/5ca5edf5bd017b9919c05d08aebef5e4c7ac3bac/modeling_florence2.py\n",
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://78268eccbd9bc4b936.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "SupervisionWarnings: from_lmm is deprecated: `Detections.from_lmm` property is deprecated and will be removed in `supervision-0.31.0`. Use Detections.from_vlm instead.\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 3113, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/florence-sam/app.py\", line 418, in <module>\n",
            "    demo.launch(share=True, debug=False, show_error=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 3019, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/blocks.py\", line 3117, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/gradio/http_server.py\", line 69, in close\n",
            "    self.thread.join(timeout=5)\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1123, in join\n",
            "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
            "  File \"/usr/lib/python3.11/threading.py\", line 1139, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 127.0.0.1:7860 <> https://78268eccbd9bc4b936.gradio.live\n"
          ]
        }
      ]
    }
  ]
}